# rss_reader.py
# ==========================================
# ä¸€ä¸ªç•Œé¢ä¼˜é›…ã€è·¨å¹³å°å¯ç”¨çš„æœ¬åœ° RSS é˜…è¯»å™¨ï¼ˆStreamlit å®ç°ï¼‰
# ç‰¹è‰²ï¼š
# - ä¾§è¾¹æ ç®¡ç†è®¢é˜…ï¼ˆæ·»åŠ /åˆ é™¤/åˆ†ç»„ï¼‰
# - å¹¶å‘æŠ“å–ã€ç¼“å­˜ï¼ˆTTL å¯è°ƒï¼‰ï¼Œç¨³å¥çš„ç½‘ç»œè¯·æ±‚ï¼ˆè¶…æ—¶/é‡è¯•/UAï¼‰
# - æœ¬åœ° SQLite æŒä¹…åŒ–ï¼ˆè®¢é˜…ã€æ–‡ç« ã€å·²è¯»/æ”¶è—ï¼‰
# - æœç´¢ã€æŒ‰æ—¶é—´æ’åºã€åªçœ‹æœªè¯»ã€åªçœ‹æ”¶è—
# - ç®€æ´å¡ç‰‡å¼ UIï¼Œè‡ªåŠ¨æå–å°é¢å›¾ä¸å›¾æ ‡
# - OPML å¯¼å…¥/å¯¼å‡º
#
# è¿è¡Œï¼š
#   1) å®‰è£…ä¾èµ–ï¼š
#      pip install -U streamlit feedparser httpx beautifulsoup4 lxml
#   2) å¯åŠ¨ï¼š
#      streamlit run rss_reader.py
#   3) Windows ç”¨æˆ·è‹¥é‡åˆ°ä¸­æ–‡è·¯å¾„é—®é¢˜ï¼Œå»ºè®®åœ¨éç³»ç»Ÿç›˜è‹±æ–‡è·¯å¾„ä¸‹è¿è¡Œã€‚
#
# å¯é€‰ç¯å¢ƒå˜é‡ï¼š
#   RSS_DB_PATH   æŒ‡å®šæ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤ ./rss_reader.dbï¼‰
#   RSS_CACHE_TTL æŠ“å–ç¼“å­˜ç§’æ•°ï¼ˆé»˜è®¤ 900 = 15 åˆ†é’Ÿï¼‰
# ==========================================

from __future__ import annotations
import os
import re
import time
import math
import json
import sqlite3
import hashlib
import asyncio
from datetime import datetime, timedelta
from urllib.parse import urlparse

import streamlit as st
import httpx
import feedparser
from bs4 import BeautifulSoup

# ------------------------- é…ç½® -------------------------
APP_TITLE = "RSS Reader"
DB_PATH = os.environ.get("RSS_DB_PATH", os.path.abspath("./rss_reader.db"))
CACHE_TTL = int(os.environ.get("RSS_CACHE_TTL", "900"))  # 15 min
REQUEST_TIMEOUT = 15.0
CONCURRENT_LIMIT = 8

DEFAULT_FEEDS = [
    {"title": "Hacker News", "url": "https://hnrss.org/frontpage", "category": "æå®¢"},
    {"title": "LWN.net", "url": "https://lwn.net/headlines/rss", "category": "å¼€æº"},
    {
        "title": "Ars Technica",
        "url": "https://feeds.arstechnica.com/arstechnica/index/",
        "category": "ç§‘æŠ€",
    },
    {
        "title": "Cloudflare Blog",
        "url": "https://blog.cloudflare.com/rss/",
        "category": "ç½‘ç»œ",
    },
    {"title": "å°‘æ•°æ´¾", "url": "https://sspai.com/feed", "category": "ä¸­æ–‡"},
    {"title": "çˆ±èŒƒå„¿", "url": "https://www.ifanr.com/feed", "category": "ä¸­æ–‡"},
    {
        "title": "Quanta Magazine",
        "url": "https://www.quantamagazine.org/feed",
        "category": "ç§‘å­¦",
    },
    {"title": "NASA APOD", "url": "https://apod.nasa.gov/apod.rss", "category": "ç§‘å­¦"},
]

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36 RSS-Reader/1.0"
)

# ------------------------- å·¥å…·å‡½æ•° -------------------------


def _connect_db():
    conn = sqlite3.connect(DB_PATH)
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("PRAGMA synchronous=NORMAL;")
    return conn


def _init_db():
    conn = _connect_db()
    cur = conn.cursor()
    cur.executescript(
        """
        CREATE TABLE IF NOT EXISTS feeds (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            url   TEXT NOT NULL UNIQUE,
            category TEXT,
            added_at TEXT DEFAULT CURRENT_TIMESTAMP
        );

        CREATE TABLE IF NOT EXISTS articles (
            id TEXT PRIMARY KEY,
            feed_id INTEGER NOT NULL,
            title TEXT,
            link  TEXT,
            author TEXT,
            summary TEXT,
            published TEXT,
            image TEXT,
            FOREIGN KEY(feed_id) REFERENCES feeds(id)
        );

        CREATE TABLE IF NOT EXISTS reads (
            article_id TEXT PRIMARY KEY,
            read_at TEXT DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY(article_id) REFERENCES articles(id)
        );

        CREATE TABLE IF NOT EXISTS stars (
            article_id TEXT PRIMARY KEY,
            starred_at TEXT DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY(article_id) REFERENCES articles(id)
        );
        """
    )
    conn.commit()
    # è‹¥ä¸ºç©ºï¼Œå†™å…¥é»˜è®¤æº
    cur.execute("SELECT COUNT(*) FROM feeds;")
    if cur.fetchone()[0] == 0:
        cur.executemany(
            "INSERT OR IGNORE INTO feeds(title, url, category) VALUES (?, ?, ?);",
            [(f["title"], f["url"], f.get("category")) for f in DEFAULT_FEEDS],
        )
        conn.commit()
    conn.close()


@st.cache_data(show_spinner=False)
def favicon_url(site_url: str) -> str:
    try:
        host = urlparse(site_url).netloc
        if not host:
            host = site_url
        # Google s2 å›¾æ ‡æœåŠ¡
        return f"https://www.google.com/s2/favicons?domain={host}&sz=64"
    except Exception:
        return ""


def _hash_id(*parts: str) -> str:
    s = "|".join([p or "" for p in parts])
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()


def _first_image_from_entry(entry) -> str | None:
    # å°è¯•å¤šç§å­—æ®µ
    for key in ("media_content", "media_thumbnail"):
        if key in entry and entry[key]:
            try:
                return entry[key][0].get("url")
            except Exception:
                pass
    # ä»å†…å®¹/summaryé‡Œæå–ç¬¬ä¸€å¼ å›¾ç‰‡
    html = ""
    if "content" in entry and entry["content"]:
        html = entry["content"][0].get("value", "")
    elif "summary" in entry:
        html = entry.get("summary", "")
    soup = BeautifulSoup(html, "lxml")
    img = soup.find("img")
    if img and img.get("src"):
        return img.get("src")
    return None


def _clean_text(x: str | None, limit=400) -> str:
    if not x:
        return ""
    txt = BeautifulSoup(x, "lxml").get_text(" ")
    txt = re.sub(r"\s+", " ", txt).strip()
    if len(txt) > limit:
        txt = txt[: limit - 1] + "â€¦"
    return txt


async def fetch_one(client: httpx.AsyncClient, url: str) -> bytes:
    r = await client.get(
        url, timeout=REQUEST_TIMEOUT, headers={"User-Agent": USER_AGENT}
    )
    r.raise_for_status()
    return r.content


@st.cache_data(ttl=CACHE_TTL, show_spinner=False)
def fetch_feed_cached(url: str) -> dict:
    """å› ä¸º Streamlit çš„ cache æ— æ³•ç›´æ¥ç¼“å­˜åç¨‹ï¼Œè¿™é‡Œåšä¸€ä¸ªåŒæ­¥å£³ã€‚"""
    return asyncio.run(_fetch_feed(url))


async def _fetch_feed(url: str) -> dict:
    async with httpx.AsyncClient(
        follow_redirects=True, headers={"User-Agent": USER_AGENT}
    ) as client:
        try:
            content = await fetch_one(client, url)
        except Exception as e:
            return {"url": url, "error": str(e), "entries": []}

    parsed = feedparser.parse(content)
    feed_title = parsed.feed.get("title", url)
    site_link = parsed.feed.get("link", url)
    entries = []
    for e in parsed.entries:
        link = getattr(e, "link", "")
        guid = getattr(e, "id", "") or link or getattr(e, "title", "")
        eid = _hash_id(url, guid, link)
        published = None
        if getattr(e, "published_parsed", None):
            published = datetime.fromtimestamp(
                time.mktime(e.published_parsed)
            ).isoformat()
        elif getattr(e, "updated_parsed", None):
            published = datetime.fromtimestamp(
                time.mktime(e.updated_parsed)
            ).isoformat()
        img = _first_image_from_entry(e)
        entries.append(
            {
                "id": eid,
                "title": getattr(e, "title", "(æ— æ ‡é¢˜)") or "(æ— æ ‡é¢˜)",
                "link": link,
                "author": getattr(e, "author", ""),
                "summary": _clean_text(getattr(e, "summary", ""), 600),
                "published": published,
                "image": img,
            }
        )
    return {"url": url, "title": feed_title, "site": site_link, "entries": entries}


def upsert_articles(feed_id: int, items: list[dict]):
    conn = _connect_db()
    cur = conn.cursor()
    cur.executemany(
        """
        INSERT OR IGNORE INTO articles(id, feed_id, title, link, author, summary, published, image)
        VALUES(:id, :feed_id, :title, :link, :author, :summary, :published, :image);
        """,
        [dict(i, feed_id=feed_id) for i in items],
    )
    conn.commit()
    conn.close()


def list_feeds() -> list[dict]:
    conn = _connect_db()
    cur = conn.cursor()
    cur.execute("SELECT id, title, url, category FROM feeds ORDER BY category, title;")
    rows = cur.fetchall()
    conn.close()
    return [{"id": r[0], "title": r[1], "url": r[2], "category": r[3]} for r in rows]


def add_feed(title: str, url: str, category: str | None):
    conn = _connect_db()
    cur = conn.cursor()
    cur.execute(
        "INSERT OR IGNORE INTO feeds(title, url, category) VALUES (?, ?, ?);",
        (title or url, url.strip(), category),
    )
    conn.commit()
    conn.close()


def remove_feed(feed_id: int):
    conn = _connect_db()
    cur = conn.cursor()
    cur.execute("DELETE FROM articles WHERE feed_id=?;", (feed_id,))
    cur.execute("DELETE FROM feeds WHERE id=?;", (feed_id,))
    conn.commit()
    conn.close()


def query_articles(
    feed_ids: list[int] | None,
    only_unread: bool,
    only_starred: bool,
    keyword: str | None,
    limit=200,
):
    conn = _connect_db()
    cur = conn.cursor()
    where = []
    params = []
    if feed_ids:
        where.append("feed_id IN (%s)" % ",".join(["?"] * len(feed_ids)))
        params.extend(feed_ids)
    if only_unread:
        where.append("articles.id NOT IN (SELECT article_id FROM reads)")
    if only_starred:
        where.append("articles.id IN (SELECT article_id FROM stars)")
    if keyword:
        where.append("(title LIKE ? OR summary LIKE ?)")
        kw = f"%{keyword}%"
        params.extend([kw, kw])
    where_sql = ("WHERE " + " AND ".join(where)) if where else ""
    cur.execute(
        f"""
        SELECT articles.id, feed_id, title, link, author, summary, published, image,
               (SELECT 1 FROM reads WHERE article_id = articles.id) as read,
               (SELECT 1 FROM stars WHERE article_id = articles.id) as starred
        FROM articles
        {where_sql}
        ORDER BY COALESCE(published, '1970-01-01') DESC
        LIMIT ?;
        """,
        (*params, limit),
    )
    rows = cur.fetchall()
    conn.close()
    return [
        {
            "id": r[0],
            "feed_id": r[1],
            "title": r[2],
            "link": r[3],
            "author": r[4],
            "summary": r[5],
            "published": r[6],
            "image": r[7],
            "read": bool(r[8]),
            "starred": bool(r[9]),
        }
        for r in rows
    ]


def mark_read(article_id: str):
    conn = _connect_db()
    conn.execute(
        "INSERT OR REPLACE INTO reads(article_id, read_at) VALUES (?, CURRENT_TIMESTAMP);",
        (article_id,),
    )
    conn.commit()
    conn.close()


def toggle_star(article_id: str):
    conn = _connect_db()
    cur = conn.cursor()
    cur.execute("SELECT 1 FROM stars WHERE article_id=?;", (article_id,))
    exist = cur.fetchone()
    if exist:
        cur.execute("DELETE FROM stars WHERE article_id=?;", (article_id,))
    else:
        cur.execute("INSERT INTO stars(article_id) VALUES (?);", (article_id,))
    conn.commit()
    conn.close()


def export_opml() -> str:
    feeds = list_feeds()
    now = datetime.utcnow().strftime("%a, %d %b %Y %H:%M:%S +0000")
    outlines = []
    for f in feeds:
        outlines.append(
            f'      <outline type="rss" text="{f["title"]}" title="{f["title"]}" xmlUrl="{f["url"]}"/>'
        )
    return (
        f'<?xml version="1.0" encoding="UTF-8"?>\n'
        f'<opml version="2.0">\n  <head>\n    <title>Exported Feeds</title>\n    <dateCreated>{now}</dateCreated>\n  </head>\n  <body>\n'
        + "\n".join(outlines)
        + "\n  </body>\n</opml>\n"
    )


def import_opml(opml_bytes: bytes) -> int:
    from xml.etree import ElementTree as ET

    root = ET.fromstring(opml_bytes)
    count = 0
    for node in root.iter("outline"):
        url = node.attrib.get("xmlUrl")
        title = node.attrib.get("title") or node.attrib.get("text")
        if url:
            try:
                add_feed(title or url, url, None)
                count += 1
            except Exception:
                pass
    return count


# ------------------------- UI éƒ¨åˆ† -------------------------

st.set_page_config(page_title=APP_TITLE, page_icon="ğŸ“°", layout="wide")

# è‡ªå®šä¹‰æ ·å¼ï¼ˆå¡ç‰‡ + ç²¾ç®€å­—ä½“ï¼‰
CARD_CSS = """
<style>
:root { --card-bg: #ffffff; --muted: #6b7280; }
[data-theme="dark"] :root { --card-bg: #111827; --muted: #9ca3af; }
.rss-card { display: grid; grid-template-columns: 140px 1fr; gap: 16px; padding: 16px; border-radius: 16px; background: var(--card-bg); box-shadow: 0 4px 20px rgba(0,0,0,0.06); margin-bottom: 14px; }
.rss-thumb { width: 140px; height: 100px; object-fit: cover; border-radius: 12px; background: #f3f4f6; }
.rss-title { font-size: 1.05rem; font-weight: 700; margin: 0 0 6px 0; line-height: 1.35; }
.rss-meta { color: var(--muted); font-size: 0.85rem; margin-bottom: 8px; }
.rss-actions { display:flex; gap:10px; align-items:center; }
.rss-summary { font-size: 0.94rem; line-height: 1.5; }
.badge { display:inline-flex; align-items:center; gap:6px; padding:2px 8px; border-radius:999px; background:#f3f4f6; color:#374151; font-size:12px; }
[data-theme="dark"] .badge { background:#1f2937; color:#d1d5db; }
</style>
"""
st.markdown(CARD_CSS, unsafe_allow_html=True)

# åˆå§‹åŒ–æ•°æ®åº“
_init_db()

# ä¾§è¾¹æ ï¼šè®¢é˜…ç®¡ç†
with st.sidebar:
    st.title("ğŸ“š è®¢é˜…ç®¡ç†")
    feeds = list_feeds()
    cats = sorted(set([f["category"] or "æœªåˆ†ç»„" for f in feeds]))
    selected_cat = st.selectbox("åˆ†ç±»ç­›é€‰", ["å…¨éƒ¨"] + cats, index=0)
    if selected_cat == "å…¨éƒ¨":
        feed_options = feeds
    else:
        feed_options = [f for f in feeds if (f["category"] or "æœªåˆ†ç»„") == selected_cat]

    feed_labels = [f"{f['title']} â€” {urlparse(f['url']).netloc}" for f in feed_options]
    selected_feed_idx = st.multiselect(
        "é€‰æ‹©è®¢é˜…ï¼ˆå¯å¤šé€‰ï¼‰",
        options=list(range(len(feed_options))),
        default=list(range(len(feed_options))),
    )
    selected_feed_ids = [feed_options[i]["id"] for i in selected_feed_idx]

    st.divider()
    st.subheader("â• æ·»åŠ è®¢é˜…")
    new_url = st.text_input("RSS/Atom åœ°å€")
    new_title = st.text_input("åç§°ï¼ˆå¯ç•™ç©ºè‡ªåŠ¨è¯†åˆ«ï¼‰")
    new_cat = st.text_input("åˆ†ç±»ï¼ˆå¯ç•™ç©ºï¼‰")
    if st.button("æ·»åŠ ", type="primary", use_container_width=True):
        if new_url.strip():
            # è¯•æŠ“ä¸€æ¬¡ï¼Œé¡ºä¾¿è¯†åˆ«æ ‡é¢˜
            info = fetch_feed_cached(new_url.strip())
            title = new_title.strip() or info.get("title") or new_url.strip()
            add_feed(title, new_url.strip(), new_cat.strip() or None)
            st.success(f"å·²æ·»åŠ ï¼š{title}")
            st.experimental_rerun()
        else:
            st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„ RSS/Atom URL")

    st.subheader("ğŸ› ï¸ OPML å¯¼å…¥/å¯¼å‡º")
    up = st.file_uploader("å¯¼å…¥ OPML", type=["opml", "xml"])
    if up is not None:
        n = import_opml(up.read())
        st.success(f"å¯¼å…¥ {n} æ¡è®¢é˜…")
        st.experimental_rerun()
    if st.button("å¯¼å‡ºå½“å‰è®¢é˜…ä¸º OPML", use_container_width=True):
        st.download_button(
            label="ä¸‹è½½ OPML",
            data=export_opml().encode("utf-8"),
            file_name="feeds_export.opml",
            mime="text/xml",
            use_container_width=True,
        )

# é¡¶éƒ¨å·¥å…·æ 
st.title(APP_TITLE)
col1, col2, col3, col4, col5 = st.columns([2, 2, 2, 2, 2])
with col1:
    keyword = st.text_input("ğŸ” æœç´¢æ ‡é¢˜/æ‘˜è¦")
with col2:
    only_unread = st.toggle("åªçœ‹æœªè¯»", value=False)
with col3:
    only_starred = st.toggle("åªçœ‹æ”¶è—", value=False)
with col4:
    limit = st.slider("æ•°é‡ä¸Šé™", 50, 500, 200, 50)
with col5:
    if st.button("ğŸ”„ ç«‹å³åˆ·æ–°", help="å¿½ç•¥ç¼“å­˜ï¼Œå¼ºåˆ¶åˆ·æ–°æ‰€é€‰è®¢é˜…"):
        # æ¸…ç©ºç¼“å­˜ï¼ˆé’ˆå¯¹ fetch_feed_cachedï¼‰
        fetch_feed_cached.clear()
        st.toast("ç¼“å­˜å·²æ¸…é™¤ï¼Œå¼€å§‹åˆ·æ–°â€¦")

# æŠ“å– & å…¥åº“ï¼ˆä»…å½“éœ€è¦æ—¶ï¼‰
if st.button("â¬‡ï¸ ä»æ‰€é€‰è®¢é˜…æ‹‰å–æœ€æ–°"):
    feeds_all = list_feeds()
    id2feed = {f["id"]: f for f in feeds_all}
    target = [id2feed[i] for i in (selected_feed_ids or [f["id"] for f in feeds_all])]
    progress = st.progress(0)
    for idx, f in enumerate(target, start=1):
        info = fetch_feed_cached(f["url"])  # æœ‰ç¼“å­˜ï¼›æ¸…ç¼“å­˜åå°±æ˜¯å®æ—¶
        if info.get("entries"):
            upsert_articles(f["id"], info["entries"])
        progress.progress(idx / len(target))
    st.success("å®Œæˆï¼")

# æŸ¥è¯¢æ–‡ç« 
articles = query_articles(
    selected_feed_ids or None,
    only_unread=only_unread,
    only_starred=only_starred,
    keyword=keyword,
    limit=limit,
)

# æ¸²æŸ“å¡ç‰‡
for a in articles:
    # æ–‡ç« å¤´éƒ¨ä¿¡æ¯
    feed = next((f for f in list_feeds() if f["id"] == a["feed_id"]), None)
    site = feed["url"] if feed else ""
    fav = favicon_url(site)

    left, right = st.columns([1, 3])
    with left:
        if a.get("image"):
            st.markdown(
                f'<img class="rss-thumb" src="{a["image"]}" alt="thumb"/>',
                unsafe_allow_html=True,
            )
        else:
            st.markdown(
                f'<img class="rss-thumb" src="{fav}" alt="thumb"/>',
                unsafe_allow_html=True,
            )

    with right:
        pub = a.get("published")
        when = "" if not pub else datetime.fromisoformat(pub).strftime("%Y-%m-%d %H:%M")
        meta = []
        if feed:
            meta.append(feed["title"])  # feed åç§°
        if when:
            meta.append(when)
        if a.get("author"):
            meta.append(a["author"])

        st.markdown(
            f"<div class='rss-card'>"
            f"<div style='grid-column: span 2;'>"
            f"<div class='rss-title'><a href='{a['link']}' target='_blank'>{a['title']}</a></div>"
            f"<div class='rss-meta'>{' Â· '.join(meta)}</div>"
            f"<div class='rss-summary'>{a['summary'] or ''}</div>"
            f"</div>"
            f"</div>",
            unsafe_allow_html=True,
        )

        c1, c2, c3 = st.columns(3)
        with c1:
            if st.button("é˜…è¯»/æ ‡è®°å·²è¯»", key=f"read-{a['id']}"):
                mark_read(a["id"])
                st.toast("å·²æ ‡è®°ä¸ºå·²è¯»")
                st.experimental_rerun()
        with c2:
            star_label = "å–æ¶ˆæ”¶è—" if a["starred"] else "æ”¶è— â­"
            if st.button(star_label, key=f"star-{a['id']}"):
                toggle_star(a["id"])
                st.experimental_rerun()
        with c3:
            if st.button("å¤åˆ¶é“¾æ¥", key=f"copy-{a['id']}"):
                st.code(a["link"], language="text")

st.caption(
    "å°æç¤ºï¼šä¾§è¾¹æ å¯æŒ‰åˆ†ç±»ç­›é€‰è®¢é˜…ï¼›é¡¶éƒ¨â€˜ç«‹å³åˆ·æ–°â€™æ¸…ç¼“å­˜ï¼Œâ€˜æ‹‰å–æœ€æ–°â€™å†™å…¥æ•°æ®åº“ï¼›å¯¼å…¥ä½ ç°æœ‰çš„ OPML å¯å¿«é€Ÿè¿ç§»ã€‚"
)
